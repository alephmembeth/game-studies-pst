{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "831e11e3",
   "metadata": {},
   "source": [
    "# Set Up Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# IMPORT #\n",
    "##########\n",
    "\n",
    "# System and file handling\n",
    "import os\n",
    "import struct\n",
    "\n",
    "# Data processing and math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Language processing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "#################\n",
    "# CONFIGURATION #\n",
    "#################\n",
    "\n",
    "# Set path\n",
    "base_path = r'/Users/amb/Desktop'\n",
    "filename = 'dialog.tlk'\n",
    "\n",
    "corpus_path = os.path.join(base_path, filename)\n",
    "\n",
    "# Configure global display settings for pandas\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Set plotting style and colors\n",
    "sns.set_style(\"whitegrid\")\n",
    "graph_color = '#2c7fb8'  \n",
    "\n",
    "# Enable inline plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f49571",
   "metadata": {},
   "source": [
    "# Parse File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bb7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses TLK file and returns DataFrame.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        \n",
    "        # Read header\n",
    "        header_data = f.read(18)\n",
    "        \n",
    "        # Unpack type, version, lang, count, offset\n",
    "        file_type, version, lang_id, string_count, string_offset = struct.unpack('<4s4sHII', header_data)\n",
    "        \n",
    "        print(f\"File Type: {file_type.decode(errors = 'ignore')}\")\n",
    "        print(f\"Version: {version.decode(errors = 'ignore')}\")\n",
    "        print(f\"Strings: {string_count}\")\n",
    "        \n",
    "        # Move to entries start\n",
    "        f.seek(18)\n",
    "        \n",
    "        # Read all entries\n",
    "        entries_data = f.read(string_count * 26)\n",
    "        \n",
    "        current_entry_idx = 0\n",
    "        while current_entry_idx < string_count:\n",
    "            chunk = entries_data[current_entry_idx * 26 : (current_entry_idx + 1) * 26]\n",
    "            \n",
    "            # Unpack flags, sound, vol, pitch, offset, length\n",
    "            flags, sound, vol, pitch, text_relative_offset, text_length = struct.unpack('<H8sIIII', chunk)\n",
    "            \n",
    "            data.append({\n",
    "                'index': current_entry_idx,\n",
    "                'offset': text_relative_offset,\n",
    "                'length': text_length\n",
    "            })\n",
    "            current_entry_idx += 1\n",
    "        \n",
    "        # Read strings\n",
    "        base_text_offset = string_offset\n",
    "        final_records = []\n",
    "        \n",
    "        for entry in data:\n",
    "            if entry['length'] > 0:\n",
    "                f.seek(base_text_offset + entry['offset'])\n",
    "                text_bytes = f.read(entry['length'])\n",
    "                try:\n",
    "                    text = text_bytes.decode('latin-1', errors = 'replace')\n",
    "                except:\n",
    "                    text = \"\"\n",
    "                \n",
    "                final_records.append({\n",
    "                    'Line': entry['index'],\n",
    "                    'Text': text\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(final_records)\n",
    "\n",
    "# Execute\n",
    "if os.path.exists(corpus_path):\n",
    "    df = read_file(corpus_path)\n",
    "    print(f\"\\nLoaded {len(df)} lines.\")\n",
    "else:\n",
    "    print(f\"File not found in {corpus_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa2078",
   "metadata": {},
   "source": [
    "# Clean Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbcd7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(text):\n",
    "    \"\"\"\n",
    "    Extracts and cleans text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\^[A-Za-z0-9\\-]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Execute\n",
    "df['Cleaned_Text'] = df['Text'].apply(clean_corpus)\n",
    "\n",
    "# Filter empty lines\n",
    "df = df[df['Cleaned_Text'].str.len() > 0].reset_index(drop = True)\n",
    "\n",
    "# Export file\n",
    "df.to_csv('game_studies_pst_corpus.csv', index = False, encoding = 'utf-8')\n",
    "\n",
    "print(\"Corpus saved to 'game_studies_pst_corpus.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6627d9",
   "metadata": {},
   "source": [
    "# Search for Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3356d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_search(df, pattern, column = 'Cleaned_Text', export_filename = \"game_studies_pst_search_results.csv\"):\n",
    "    \"\"\"\n",
    "    Scans the DataFrame for keywords and returns hits.\n",
    "    \"\"\"\n",
    "    regex_pattern = pattern.replace('*', '.*').replace('?', '.')\n",
    "    \n",
    "    matches = df[df[column].str.contains(regex_pattern, case = False, regex = True)]\n",
    "    \n",
    "    print(f\"{len(matches)} matches found.\")\n",
    "    \n",
    "    if export_filename:\n",
    "        if not export_filename.endswith('.csv'):\n",
    "            export_filename += '.csv'\n",
    "        \n",
    "        matches.to_csv(export_filename, index = False, encoding = 'utf-8')\n",
    "        print(f\"Results exported to '{export_filename}'.\")\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Execute\n",
    "results = run_search(df, \"nameless\")\n",
    "\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f699aaa8",
   "metadata": {},
   "source": [
    "# Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df):\n",
    "    # Tokenization\n",
    "    df['Tokens'] = df['Cleaned_Text'].apply(word_tokenize)\n",
    "    \n",
    "    # Flatten list of tokens\n",
    "    all_tokens = [word.lower() for sublist in df['Tokens'] for word in sublist if word.isalnum()]\n",
    "    \n",
    "    # Calculate lines, characters, and tokens\n",
    "    total_lines = len(df)\n",
    "    total_chars = df['Cleaned_Text'].str.len().sum()\n",
    "    total_tokens = len(all_tokens)\n",
    "    \n",
    "    # Calculate unique types\n",
    "    freq_dist = FreqDist(all_tokens)\n",
    "    unique_types = len(freq_dist)\n",
    "    \n",
    "    # Calculate type-token ratio\n",
    "    ttr = unique_types / total_tokens if total_tokens > 0 else 0\n",
    "    \n",
    "    # Calculate hapax legomena\n",
    "    hapax_legomena = [word for word, count in freq_dist.items() if count == 1]\n",
    "    hapax_count = len(hapax_legomena)\n",
    "    percentage_hapax = (hapax_count / unique_types) * 100 if unique_types > 0 else 0\n",
    "    \n",
    "    # Calculate average line length\n",
    "    df['Line_Length_Tokens'] = df['Tokens'].apply(len)\n",
    "    avg_line_length = df['Line_Length_Tokens'].mean()\n",
    "    \n",
    "    # Calculate average word length\n",
    "    word_lengths = [len(w) for w in all_tokens]\n",
    "    avg_word_length = np.mean(word_lengths) if word_lengths else 0\n",
    "    \n",
    "    # Calculate lexical density\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    content_words = [w for w in all_tokens if w not in stop_words]\n",
    "    lexical_density = (len(content_words) / total_tokens) * 100 if total_tokens > 0 else 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Total Lines: {total_lines}\")\n",
    "    print(f\"Total Characters: {total_chars}\")\n",
    "    print(f\"Total Tokens: {total_tokens}\")\n",
    "    print(f\"Unique Types: {unique_types}\")\n",
    "    print(f\"Type-Token Ratio: {ttr:.4f}\")\n",
    "    print(f\"Hapax Legomena: {hapax_count}\")\n",
    "    print(f\"Percentage of Hapax Legomena: {percentage_hapax:.2f}%\")\n",
    "    print(f\"Average Line Length: {avg_line_length:.2f}\")\n",
    "    print(f\"Average Word Length: {avg_word_length:.2f}\")\n",
    "    print(f\"Lexical Density: {lexical_density:.2f}%\")\n",
    "    \n",
    "    return all_tokens, freq_dist, word_lengths\n",
    "\n",
    "# Execute\n",
    "all_tokens, freq_dist, word_lengths = calculate_metrics(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead3ec16",
   "metadata": {},
   "source": [
    "# Calculate Top Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c10225",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = list(ngrams(all_tokens, 2))\n",
    "bigram_freq = Counter(bigrams)\n",
    "\n",
    "for bg, count in bigram_freq.most_common(25):\n",
    "    print(f\"{bg}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9aef6",
   "metadata": {},
   "source": [
    "# Calculate Word Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db8d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_length_counts = Counter(word_lengths)\n",
    "sorted_word_lengths = sorted(word_length_counts.items())\n",
    "\n",
    "for length, count in sorted_word_lengths[:12]:\n",
    "    print(f\"{length}: {count}\")\n",
    "\n",
    "print(f\"\\nUnique Lengths: {len(sorted_word_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61971bd1",
   "metadata": {},
   "source": [
    "# Plot Word Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2579d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_length_distribution(word_lengths, color = graph_color, export_filename = \"game_studies_pst_word_length_distribution.png\"):\n",
    "    \"\"\"\n",
    "    Creates a bar chart for word length distribution.\n",
    "    \"\"\"\n",
    "    # Check data\n",
    "    if not word_lengths:\n",
    "        print(\"No data found.\")\n",
    "        return\n",
    "    \n",
    "    # Filter word lengths\n",
    "    word_length_counts = Counter(word_lengths)\n",
    "    \n",
    "    filtered_counts = {length: count for length, count in word_length_counts.items() if 1 <= length <= 12}\n",
    "    \n",
    "    if not filtered_counts:\n",
    "        print(\"No data found.\")\n",
    "        return\n",
    "    \n",
    "    # Sort data\n",
    "    sorted_word_lengths = sorted(filtered_counts.items())\n",
    "    x_vals, y_vals = zip(*sorted_word_lengths)\n",
    "    \n",
    "    # Plot figure\n",
    "    plt.figure(figsize = (12, 6))\n",
    "    \n",
    "    sns.barplot(x = list(x_vals), y = list(y_vals), color = color)\n",
    "    \n",
    "    plt.xlabel(\"Characters per Word\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    # Export graph\n",
    "    if export_filename:\n",
    "        if not export_filename.endswith('.png'):\n",
    "            export_filename += '.png'\n",
    "        plt.savefig(export_filename, dpi = 300, bbox_inches = 'tight')\n",
    "        print(f\"Graph saved as '{export_filename}'.\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Execute\n",
    "plot_word_length_distribution(word_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1e13b",
   "metadata": {},
   "source": [
    "# Plot Barcode for Selected Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e699a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_barcode_keywords(df, keywords, column = 'Cleaned_Text', color = graph_color, export_filename = \"game_studies_pst_barcode_keywords.png\"):\n",
    "    \"\"\"\n",
    "    Creates a barcode of selected keywords.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot figure\n",
    "    plt.figure(figsize = (15, len(keywords) * 0.8))\n",
    "    \n",
    "    for i, keyword in enumerate(keywords):\n",
    "        if '*' in keyword or '?' in keyword:\n",
    "            regex_pattern = re.escape(keyword).replace(r'\\*', '.*').replace(r'\\?', '.')\n",
    "            matches = df[df[column].str.contains(regex_pattern, case = False, regex = True)].index\n",
    "        else:\n",
    "            matches = df[df[column].str.contains(r'\\b' + re.escape(keyword) + r'\\b', case = False, regex = True)].index\n",
    "        \n",
    "        plt.vlines(matches, i, i + 1, colors = [color], linewidth = 0.8)\n",
    "        plt.text(-len(df) * 0.02, i + 0.5, keyword, va = 'center', fontweight = 'bold', ha = 'right')\n",
    "    \n",
    "    plt.xlabel(\"Line in Corpus\")\n",
    "    plt.yticks([])\n",
    "    plt.xlim(0, len(df))\n",
    "    plt.ylim(0, len(keywords))\n",
    "    \n",
    "    # Export graph\n",
    "    if export_filename:\n",
    "        if not export_filename.endswith('.png'):\n",
    "            export_filename += '.png'\n",
    "        plt.savefig(export_filename, dpi = 300, bbox_inches = 'tight')\n",
    "        print(f\"Graph saved as '{export_filename}'.\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Set keywords\n",
    "keywords = ['death', 'life', 'nameless', 'morte']\n",
    "\n",
    "# Execute\n",
    "plot_barcode_keywords(df, keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
